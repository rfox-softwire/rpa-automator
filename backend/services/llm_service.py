from typing import Dict, Any, Optional
import json
from pathlib import Path
import uuid

from backend.models.base import InstructionRequest, LLMResponse
from backend.prompts import get_generate_prompt, get_repair_prompt

class LLMService:
    def __init__(self, llm_client, scripts_dir: str = "data/scripts", prompts_dir: str = "data/prompts"):
        self.llm_client = llm_client
        # Set up scripts directory
        self.scripts_dir = Path(scripts_dir)
        self.scripts_dir.mkdir(exist_ok=True, parents=True)
        # Set up prompts directory
        self.prompts_dir = Path(prompts_dir)
        self.prompts_dir.mkdir(exist_ok=True, parents=True)
    
    async def generate_script(self, instruction: InstructionRequest) -> Dict[str, Any]:
        """Generate a script based on user instruction."""
        try:
            # Generate the appropriate prompt
            if instruction.error_context and instruction.original_script:
                prompt = get_repair_prompt(
                    instruction=instruction.content,
                    error_context=instruction.error_context,
                    original_script=instruction.original_script
                )
                is_repair = True
            else:
                prompt = get_generate_prompt(instruction.content)
                is_repair = False
            
            # Get the LLM response
            llm_response = await self.llm_client.generate_text(
                prompt=prompt,
                max_tokens=-1,  # -1 means use maximum context length
                temperature=0.7  # Default temperature
            )

            # Log the full response for debugging
            print(f"LLM Response Type: {type(llm_response).__name__}")
            print(f"LLM Response Keys: {list(llm_response.keys()) if hasattr(llm_response, 'keys') else 'Not a dictionary'}")
            
            # Extract the generated Python code
            try:
                if not isinstance(llm_response, dict):
                    raise ValueError(f"Expected dictionary response, got {type(llm_response).__name__}")
                    
                choices = llm_response.get('choices', [])
                if not choices:
                    raise ValueError("No choices in LLM response")
                    
                first_choice = choices[0]
                if not isinstance(first_choice, dict):
                    raise ValueError(f"Expected choice to be a dictionary, got {type(first_choice).__name__}")
                    
                message = first_choice.get('message', {})
                if not isinstance(message, dict):
                    raise ValueError(f"Expected message to be a dictionary, got {type(message).__name__}")
                    
                generated_content = message.get('content', '')
                if not generated_content:
                    raise ValueError("No content in LLM response")
                
                print(f"Generated content length: {len(generated_content)} characters")
                print(f"Generated content preview: {generated_content[:200]}...")
                
                # Try to extract Python code block
                if '```python' in generated_content:
                    parts = generated_content.split('```python')
                    if len(parts) < 2:
                        raise ValueError("Could not find closing code block marker after ```python")
                    python_script = parts[1].split('```')[0].strip()
                elif '```' in generated_content:
                    parts = generated_content.split('```')
                    if len(parts) < 2:
                        raise ValueError("Could not find closing code block marker after ```")
                    python_script = parts[1].strip()
                    if python_script.startswith('python\n'):
                        python_script = python_script[7:].strip()
                else:
                    python_script = generated_content.strip()
                
                if not python_script:
                    raise ValueError("No code was generated by the LLM")
                    
                # Ensure scripts directory exists
                self.scripts_dir.mkdir(parents=True, exist_ok=True)
                self.prompts_dir.mkdir(parents=True, exist_ok=True)
                
                # Save the generated script
                script_id = str(uuid.uuid4())
                script_filename = f"script_{script_id}.py"
                script_path = self.scripts_dir / script_filename
                
                try:
                    # Write the script file
                    script_path.write_text(python_script, encoding='utf-8')
                    print(f"Successfully saved script to: {script_path.absolute()}")
                    
                    # Save the prompt used to generate the script
                    prompt_filename = f"prompt_{script_id}.txt"
                    prompt_path = self.prompts_dir / prompt_filename
                    prompt_path.write_text(prompt, encoding='utf-8')
                    print(f"Successfully saved prompt to: {prompt_path.absolute()}")
                    
                    # Verify the file was written
                    if not script_path.exists():
                        raise IOError(f"Failed to verify script file creation at {script_path}")
                    
                    return {
                        "status": "success",
                        "message": "Script created",
                        "script_path": str(script_path.absolute()),
                        "script_content": python_script,
                        "is_repair": is_repair,
                        "prompt_used": prompt
                    }
                    
                except Exception as file_error:
                    error_msg = f"Error saving files: {str(file_error)}"
                    print(error_msg)
                    # Try to clean up partially written files
                    if script_path.exists():
                        try:
                            script_path.unlink()
                        except Exception as cleanup_error:
                            print(f"Error cleaning up script file: {cleanup_error}")
                    if 'prompt_path' in locals() and prompt_path.exists():
                        try:
                            prompt_path.unlink()
                        except Exception as cleanup_error:
                            print(f"Error cleaning up prompt file: {cleanup_error}")
                    raise
                
            except Exception as e:
                error_msg = f"Error processing LLM response: {str(e)}"
                print(f"{error_msg}\nResponse content: {generated_content}")
                return {"error": error_msg, "success": False, "response_content": generated_content}
        except Exception as e:
            error_msg = f"Error generating script: {str(e)}"
            print(error_msg)
            return {"error": error_msg, "success": False, "details": str(e)}
